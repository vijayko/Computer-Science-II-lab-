{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab 3: Using Multiple Numerical Features and Feature Scaling",
      "provenance": [],
      "collapsed_sections": [
        "JndnmDMp66FL"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vijayko/Computer-Science-II-lab-/blob/master/Lab_3_Using_Multiple_Numerical_Features_and_Feature_Scaling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JndnmDMp66FL"
      },
      "source": [
        "#### Copyright 2018 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "hMqWDc_m6rUC",
        "colab": {}
      },
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lab_objectives"
      },
      "source": [
        "# Lab 3: Using Multiple Numerical Features and Feature Scaling\n",
        "** Learning Objectives **\n",
        "* Train a model using more than one feature and use a calibration plot to visualize the model quality.\n",
        "* Learn the importance of feature transformations.\n",
        "* Introduce linear and log transformations of features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "combined_import_panda_text"
      },
      "source": [
        "### Imports and Pandas Options\n",
        "We import the libraries we are using and set some panda options."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "combined_import_panda_code",
        "colab": {}
      },
      "source": [
        "import fnmatch\n",
        "import math\n",
        "\n",
        "from IPython import display\n",
        "from matplotlib import cm\n",
        "from matplotlib import gridspec\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.data import Dataset\n",
        "\n",
        "# This line increasing the amount of logging when there is an error.  You can\n",
        "# remove it if you want less logging\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "# Set the output display to have two digits for decimal places, for display\n",
        "# readability only and limit it to printing 15 rows.\n",
        "pd.options.display.float_format = '{:.2f}'.format\n",
        "pd.options.display.max_rows = 15"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "data_set_text_short_description"
      },
      "source": [
        "### Data Set\n",
        "As in the last lab we use the [Automobile Data Set](https://archive.ics.uci.edu/ml/datasets/automobile)  from 1985 Ward's Automotive Yearbook that is part of the  [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "load_auto_data_set_text"
      },
      "source": [
        "### Loading and Randomizing the Data\n",
        "Load the data using the column names from [Automobile Data Set](https://archive.ics.uci.edu/ml/datasets/automobile). When using SGD (stochastic graident descent) for training it is important that **each batch is a random sample of the data** so that the gradient computed is representative.  While there appears to be no order to this data set, it is always good practice to shuffle the data to be in a random order.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "load_auto_data_set_code",
        "colab": {}
      },
      "source": [
        "# Provide the names for the columns since the CSV file with the data does\n",
        "# not have a header row.\n",
        "cols = ['symboling', 'losses', 'make', 'fuel-type', 'aspiration', 'num-doors',\n",
        "        'body-style', 'drive-wheels', 'engine-location', 'wheel-base',\n",
        "        'length', 'width', 'height', 'weight', 'engine-type', 'num-cylinders',\n",
        "        'engine-size', 'fuel-system', 'bore', 'stroke', 'compression-ratio',\n",
        "        'horsepower', 'peak-rpm', 'city-mpg', 'highway-mpg', 'price']\n",
        "\n",
        "\n",
        "# Load in the data from a CSV file that is comma seperated.\n",
        "car_data = pd.read_csv('https://storage.googleapis.com/ml_universities/cars_dataset/cars_data.csv',\n",
        "                        sep=',', names=cols, header=None, encoding='latin-1')\n",
        "\n",
        "# We'll then randomize the data, just to be sure not to get any pathological\n",
        "# ordering effects that might harm the performance of Stochastic Gradient\n",
        "# Descent.\n",
        "car_data = car_data.reindex(np.random.permutation(car_data.index))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "convert_missing_to_mean_text"
      },
      "source": [
        "### Converting Missing Numerical Values to the Column Mean\n",
        "\n",
        "As you hopefully found in a previous exercise, a good option for replacing missing entries (NaN) is to replace them by the column mean.  We do that here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "convert_missing_to_mean_code",
        "colab": {}
      },
      "source": [
        "car_data['price'] = pd.to_numeric(car_data['price'], errors='coerce')\n",
        "car_data['horsepower'] = pd.to_numeric(car_data['horsepower'], errors='coerce')\n",
        "car_data['peak-rpm'] = pd.to_numeric(car_data['peak-rpm'], errors='coerce')\n",
        "car_data['city-mpg'] = pd.to_numeric(car_data['city-mpg'], errors='coerce')\n",
        "car_data['highway-mpg'] = pd.to_numeric(car_data['highway-mpg'], errors='coerce')\n",
        "\n",
        "# Replace nan by the mean storing the solution in the same table (`inplace')\n",
        "car_data.fillna(car_data.mean(), inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "prepare_features_text"
      },
      "source": [
        "###Prepare Features\n",
        "As our learning models get more sophisticated we will want to do some computation on the features and even generate new features from the existing ones. We will see examples of this in later labs.  For now this method will just make a copy of a portion of the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "prepare_features_code",
        "colab": {}
      },
      "source": [
        "def prepare_features(dataframe):\n",
        "  \"\"\"Prepares the features for provided dataset.\n",
        "\n",
        "  Args:\n",
        "    dataframe: A Pandas DataFrame expected to contain data from the\n",
        "      desired data set.\n",
        "  Returns:\n",
        "    A new dataFrame that contains the features to be used for the model.\n",
        "  \"\"\"\n",
        "  processed_features = dataframe.copy()\n",
        "  return processed_features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "generate_training_examples_text"
      },
      "source": [
        "###Generate the Training Examples\n",
        "We simply call `prepare_features` on the `car_data` dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "generate_training_examples_code",
        "colab": {}
      },
      "source": [
        "training_examples = prepare_features(car_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "setting_up_numerical_feature_columns_text"
      },
      "source": [
        "###Setting Up the Feature Columns for TensorFlow\n",
        "\n",
        "In order to import our training data into TensorFlow, we need to specify what type of data each feature contains. There are two main types of data we'll use in this and future exercises:\n",
        "\n",
        "* **Numerical Data**: Data that is a number (integer or float) and that you want to treat as a number. As we will discuss more later, sometimes you might want to treat numerical data (e.g., a postal code) as if it were categorical.\n",
        "\n",
        "* **Categorical Data**: Data that is textual such as `make` or `fuel-type`.\n",
        "\n",
        "In TensorFlow, we indicate a feature's data type using a construct called a **feature column**. Feature columns store only a description of the feature data; they do not contain the feature data itself.\n",
        "\n",
        "For now, we will just use numerical features.  Later you will learn how to use categorical data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "setting_up_numerical_feature_columns_code",
        "colab": {}
      },
      "source": [
        "NUMERICAL_FEATURES = [\"horsepower\"]\n",
        "\n",
        "def construct_feature_columns():\n",
        "  \"\"\"Construct the TensorFlow Feature Columns.\n",
        "\n",
        "  Returns:\n",
        "    A set of feature columns\n",
        "  \"\"\" \n",
        "  return set([tf.feature_column.numeric_column(feature)\n",
        "              for feature in NUMERICAL_FEATURES])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "input_function_text"
      },
      "source": [
        "### Input Function\n",
        "To import our data into a LinearRegressor, we need to define an input function, which instructs TensorFlow how to preprocess the data, as well as how to batch, shuffle, and repeat it during model training.\n",
        "\n",
        "First, we'll convert our Pandas feature data into a dictionary of NumPy arrays. We can then use the TensorFlow Dataset API to construct a dataset object from our data, and then break our data into batches of batch_size, to be repeated for the specified number of epochs (num_epochs).\n",
        "\n",
        "When the default value of num_epochs=None is passed to [Dataset.repeat()](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#repeat), the input data will be repeated indefinitely.\n",
        "\n",
        "Next, if shuffle is set to True, we'll shuffle the data so that it's passed to the model randomly during training. We'll shuffle the data using [Dataset.shuffle()](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle), which receives a parameter buffer_size that specifies the size of the dataset from which shuffle will randomly sample.\n",
        "\n",
        "Finally, our input function constructs an iterator for the dataset and returns the next batch of data to the LinearRegressor.\n",
        "\n",
        "**NOTE:** We'll continue to use this same input function in later exercises. For more\n",
        "detailed documentation of input functions and the `Dataset` API, see the [TensorFlow Programmer's Guide](https://www.tensorflow.org/programmers_guide/datasets)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "input_function_code",
        "colab": {}
      },
      "source": [
        "def input_fn(features, targets, batch_size=1, shuffle=True, num_epochs=None):\n",
        "    \"\"\"Defines a function to preprocess the data, as well as how to batch,\n",
        "      shuffle, and repeat it during model training..\n",
        "  \n",
        "    Args:\n",
        "      features: pandas DataFrame of features\n",
        "      targets: pandas DataFrame of targets\n",
        "      batch_size: Size of batches to be passed to the model\n",
        "      shuffle: True or False. Whether to shuffle the data.\n",
        "      num_epochs: Number of epochs for which data should be repeated. None = repeat indefinitely\n",
        "    Returns:\n",
        "      Tuple of (features, labels) for next data batch\n",
        "    \"\"\"\n",
        "    \n",
        "    # Convert pandas data into a dict of np arrays.\n",
        "    features = {key:np.array(value) for key,value in dict(features).items()}                                           \n",
        " \n",
        "    # Construct a dataset, and configure batching/repeating.\n",
        "    ds = Dataset.from_tensor_slices((features,targets)) # warning: 2GB limit\n",
        "    ds = ds.repeat(num_epochs)\n",
        "    if shuffle:\n",
        "      ds = ds.shuffle(10000)\n",
        "    ds = ds.batch(batch_size)\n",
        "    \n",
        "    # Return the next batch of data.\n",
        "    features, labels = ds.make_one_shot_iterator().get_next()\n",
        "    return features, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "configure_linear_regressor_text"
      },
      "source": [
        "###Configure the LinearRegressor\n",
        "\n",
        "Next, we'll configure a linear regression model using LinearRegressor. We'll train this model using the `GradientDescentOptimizer`, which implements Mini-Batch Stochastic Gradient Descent (SGD). The `learning_rate` argument controls the size of the gradient step.\n",
        "\n",
        "**NOTE:** To be safe, we also apply [gradient clipping](https://developers.google.com/machine-learning/glossary/#gradient_clipping) to our optimizer via `clip_gradients_by_norm`. Gradient clipping ensures the magnitude of the gradients do not become too large during training, which can cause gradient descent to fail. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "configure_linear_regressor_code",
        "colab": {}
      },
      "source": [
        "def define_linear_regression_model(learning_rate):\n",
        "  \"\"\" Defines a linear regression model of one feature to predict the target.\n",
        "  \n",
        "  Args:\n",
        "    learning_rate: A `float`, the learning rate\n",
        "    \n",
        "  Returns:\n",
        "    A linear regressor created with the given learning rate\n",
        "  \"\"\"\n",
        "  \n",
        "  optimizer=tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
        "  optimizer = tf.contrib.estimator.clip_gradients_by_norm(optimizer, 5.0)\n",
        "  linear_regressor = tf.estimator.LinearRegressor(\n",
        "    feature_columns=construct_feature_columns(),\n",
        "    optimizer=optimizer\n",
        "  )  \n",
        "  return linear_regressor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "train_model_text"
      },
      "source": [
        "###Train the Model\n",
        "\n",
        "We now have all the pieces we need to train a model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "train_model_code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "1ef8f7f5-3053-46f2-cf79-9bae22e83c2f"
      },
      "source": [
        "NUMERICAL_FEATURES = [\"horsepower\"]\n",
        "CATEGORICAL_FEATURES = []\n",
        "LABEL = \"price\"\n",
        "\n",
        "# Create regression model using the define_regression_model procedure that we\n",
        "# defined earlier.\n",
        "linear_regressor = define_linear_regression_model(learning_rate = 1)\n",
        "\n",
        "train_input_fn = lambda: input_fn(training_examples[NUMERICAL_FEATURES], \n",
        "                                  training_examples[LABEL], \n",
        "                                  batch_size=50)\n",
        "\n",
        "# Train the predictor using 100 steps through the data.\n",
        "_ = linear_regressor.train(\n",
        "      input_fn=train_input_fn, steps=100\n",
        ")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Entity <bound method LinearModel.call of <tensorflow.python.feature_column.feature_column_v2.LinearModel object at 0x7fdff24c8cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LinearModel.call of <tensorflow.python.feature_column.feature_column_v2.LinearModel object at 0x7fdff24c8cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method _LinearModelLayer.call of <tensorflow.python.feature_column.feature_column_v2._LinearModelLayer object at 0x7fdff1bc2390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method _LinearModelLayer.call of <tensorflow.python.feature_column.feature_column_v2._LinearModelLayer object at 0x7fdff1bc2390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "compute_loss_text"
      },
      "source": [
        "### Computing the Loss\n",
        "For now we are using root mean squared error (RMSE) for our loss since that is the appropriate loss to use for linear regression.  However, to keep the procedure to train the model very generic, we introduce a method to compute loss that can be tailored to other types of problems. For this lab, our implementation will be to return the RMSE.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "compute_loss_code",
        "colab": {}
      },
      "source": [
        "def compute_loss(predictions, targets):\n",
        "  \"\"\" Computes the loss (RMSE) for linear regression.\n",
        "  \n",
        "  Args:\n",
        "    predictions: a list of values predicted by the model being visualized\n",
        "    targets: a list of the target values being predicted that must be the\n",
        "             same size as predictions.\n",
        "    \n",
        "  Returns:\n",
        "    The RMSE for the provided predictions and targets\n",
        "  \"\"\"      \n",
        "  return math.sqrt(metrics.mean_squared_error(predictions, targets))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "learning_curve_text"
      },
      "source": [
        "###Learning Curve\n",
        "\n",
        "Another important tool is a graph often called a **learning curve** that shows the loss being minimized on the y-axis and the training steps (time) on the x-axis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "learning_curve_code",
        "colab": {}
      },
      "source": [
        "def plot_learning_curve(training_losses):\n",
        "  \"\"\" Plot the learning curve\n",
        "  \n",
        "  Args:\n",
        "    training_loses: a list of losses to plot\n",
        "  \"\"\"        \n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('Training Steps')\n",
        "  plt.plot(training_losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "calibration_plot_text"
      },
      "source": [
        "###Visualizing Our Model Via a Calibration Plot\n",
        "\n",
        "When we just use a single input feature, we can visualize the data and the learned model very well with **scatter plot**.   The scatter plots we've been looking at previously plot a feature value (x-axis) against the target value (y-axis). It also plots the line which shows the model, that is the line of best fit learned by linear regression.\n",
        "\n",
        "In order to help understand higher-dimensional models, we use a  **calibration plot** that shows the models' predictions (x-axis) vs. the true values (y-axis). We also plot the line x = y (target = prediction), which corresponds to a model with a RMSE of zero. For points that are under the line, we are overpredicting and points over the line we are underpredicting.\n",
        "\n",
        "Observe that unlike in the scatter plot where the points are fixed and the model (as viewed as a line changes), in the callibration plot the x-coordinate of the points change as the model is trained."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "calibration_plot_code",
        "colab": {}
      },
      "source": [
        "def calibration_plot(predictions, targets):\n",
        "  \"\"\" Creates a calibration plot.\n",
        "  \n",
        "  Args:\n",
        "    predictions: a list of values predicted by the model being visualized\n",
        "    targets: a list of the target values being predicted that must be the\n",
        "             same size as predictions.\n",
        "  \"\"\"  \n",
        "  calibration_data = pd.DataFrame()\n",
        "  calibration_data[\"predictions\"] = pd.Series(predictions)\n",
        "  calibration_data[\"targets\"] = pd.Series(targets)\n",
        "  min_val = calibration_data[\"predictions\"].min()\n",
        "  max_val = calibration_data[\"predictions\"].max()\n",
        "  plt.ylabel(\"target\")\n",
        "  plt.xlabel(\"prediction\")\n",
        "  plt.scatter(predictions, targets, color='black')\n",
        "  plt.plot([min_val, max_val], [min_val, max_val])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "studying_calibration_plot_text"
      },
      "source": [
        "## Exercise: Studying the Calibration Plot (1 point)\n",
        "\n",
        "The code below generates a calibration plot.  Please answer the questions below to help you get a better understanding of the calibration plot and how it is different from a scatter plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "studying_calibration_plot_code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "outputId": "52cc802d-dad1-49c2-cd05-46f4ed30109d"
      },
      "source": [
        "# Call predict() on the linear_regressor to make predictions.\n",
        "training_predict_input_fn = lambda: input_fn(\n",
        "    training_examples[NUMERICAL_FEATURES], \n",
        "    training_examples[LABEL], num_epochs=1, shuffle=False)\n",
        "predictions = linear_regressor.predict(input_fn=training_predict_input_fn)\n",
        "predictions = np.array([item['predictions'][0] for item in predictions])\n",
        "calibration_plot(predictions, car_data[LABEL].values)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Entity <bound method LinearModel.call of <tensorflow.python.feature_column.feature_column_v2.LinearModel object at 0x7fdff66eea58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LinearModel.call of <tensorflow.python.feature_column.feature_column_v2.LinearModel object at 0x7fdff66eea58>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method _LinearModelLayer.call of <tensorflow.python.feature_column.feature_column_v2._LinearModelLayer object at 0x7fdff0ff9588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method _LinearModelLayer.call of <tensorflow.python.feature_column.feature_column_v2._LinearModelLayer object at 0x7fdff0ff9588>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEKCAYAAADaa8itAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "exercise_calibration_plot",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "The calibration plot shows the models' predictions (x-axis) vs. the true values\n",
        "(y-axis). It also plots the line x = y.\n",
        "\n",
        "The scatter plots we've been looking at previously plot a feature value (x-axis)\n",
        "against the target value (y-axis). It also plots the line which shows the model,\n",
        "that is the line-of-best-fitted learned by linear regression.\n",
        "\n",
        "TYPE YOUR ANSWERS TO THESE QUESTIONS IN THIS COMMENT BOX\n",
        "\n",
        "- How can you tell a model is performing well or poorly by looking at the\n",
        "  calibration plot? How about for the scatter plot?\n",
        "  Ans: If the data points are aligned or closer to the diagonal (y = x) line in the calibration, the model is performing well, otherwise, the model is not predicting \n",
        "  well with larger error. It is intuitive with the calibration plot, since, ideally, the targeted values should be the predicted value if the model is perfect. On the other\n",
        "  hand, the scatter plot is simply plotting the data points, from which we can estimate the nature of the correlation between the independent and dependent features, \n",
        "  and the distribution of the data points. \n",
        "  \n",
        "- Which one do you think would be more useful for evaluating how well the model\n",
        "  is working?\n",
        "  Ans: Calibration plot is a better measure to evaluate the performance of the model. \n",
        "  \n",
        "- Can you plot a feature vs. target scatter plot (as we've done in past labs)\n",
        "  with multiple feature values? How about a calibration plot?\n",
        "  \n",
        "- What does the line x = y in the calibration plot help you determine?\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "study_our_data_text"
      },
      "source": [
        "### Studying Our Data\n",
        "\n",
        "It is always valuable to understand you data.  Let's look at a histogram showing the values of `price`, `highway-mpg`, and `city-mpg`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "study_our_data_code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(20, 5))\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.title(\"price\")\n",
        "histogram = car_data[\"price\"].hist(bins=50)\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.title(\"highway-mpg\")\n",
        "histogram = car_data[\"highway-mpg\"].hist(bins=50)\n",
        "\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.title(\"city-mpg\")\n",
        "histogram = car_data[\"city-mpg\"].hist(bins=50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "train_model_function_text"
      },
      "source": [
        "### Function to Train a Model\n",
        "\n",
        "We provide a method that trains a linear model using any set of features. This method divides the learning up into 10 periods, and shows the learning curve and calibration plot after training the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "train_model_function_code",
        "colab": {}
      },
      "source": [
        "# Function to train any linear model using a calibration plot to help visualize\n",
        "# the final model\n",
        "\n",
        "def train_model(linear_regressor, features, labels, steps, batch_size):\n",
        "  \"\"\"Trains a linear regression model.\n",
        "  \n",
        "  Args:\n",
        "    linear_regressor: The regressor to train\n",
        "    features: The input features to use\n",
        "    label: the labels\n",
        "    steps: A non-zero `int`, the total number of training steps.\n",
        "    batch_size: A non-zero `int`, the batch size.\n",
        "    \n",
        "  Returns:\n",
        "    The trained regressor\n",
        "  \"\"\"\n",
        "  # In order to see how the model evolves as we train it, we will divide the\n",
        "  # steps into periods and show the model after each period.\n",
        "  periods = 10\n",
        "  steps_per_period = steps / periods\n",
        "  \n",
        "  # Set up the training_input_fn and predict_training_input_fn\n",
        "  training_input_fn = lambda: input_fn(features, labels, batch_size=batch_size)\n",
        "  predict_training_input_fn = lambda: input_fn(features, labels, num_epochs=1,\n",
        "                                               shuffle=False)\n",
        "  \n",
        "  # Train the model, but do so inside a loop so that we can periodically assess\n",
        "  # loss metrics.  We store the training losses to generate the training curve\n",
        "  print(\"Training model...\")\n",
        "  training_losses = []\n",
        "\n",
        "  for period in range (0, periods):\n",
        "    # Call fit to train the regressor for steps_per_period steps\n",
        "    _ = linear_regressor.train(input_fn=training_input_fn, steps=steps_per_period)\n",
        "\n",
        "    # Use the predict method to compute the predictions from the current model\n",
        "    predictions = linear_regressor.predict(input_fn=predict_training_input_fn)\n",
        "    predictions = np.array([item['predictions'][0] for item in predictions])\n",
        "   \n",
        "    # Compute the loss between the predictions and the correct labels, append\n",
        "    # the loss to the list of losses used to generate the learning curve after\n",
        "    # training is complete and print the current loss\n",
        "    loss = compute_loss(predictions, labels)\n",
        "    training_losses.append(loss) \n",
        "    print(\"  Loss after period %02d : %0.3f\" % (period, loss))\n",
        "      \n",
        "  # Now that training is done print the final loss    \n",
        "  print(\"Final Loss (RMSE) on the training data: %0.3f\" % loss) \n",
        "  \n",
        "  # Generate a figure with the learning curve on the left, and a calibration\n",
        "  # plot on the right.\n",
        "  plt.figure(figsize=(10, 5))\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.title(\"Learning Curve (RMSE vs time)\")\n",
        "  plot_learning_curve(training_losses)\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.tight_layout(pad=1.1, w_pad=3.0, h_pad=3.0)\n",
        "  plt.title(\"Calibration Plot\")\n",
        "  calibration_plot(predictions, labels)\n",
        "   \n",
        "  return linear_regressor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "train_model_two_features_text"
      },
      "source": [
        "##Exercise: Train a Model Using Multiple Features (1 point)\n",
        "\n",
        "The focus on this section is learning some of the issues that arise,  and how to address them when you train a model with multiple features.  The first task is to train a model to predict `city-mpg` from `highway-mpg` and `price` without using any feature processing.  Remember what you learned in the last lab about how to find a good learning rate and numer of steps to train.  **Only spend 15 mins on this excercise.  You are likely to find that it will be very hard to train a model and the goal here is to recognize this and the reason why.  So do not worry if after 15 mins you do not have a very good model.  Move on to the next exercise.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "exercise_train_model_two_features",
        "colab": {}
      },
      "source": [
        "NUMERICAL_FEATURES = [\"price\", \"highway-mpg\"]\n",
        "CATEGORICAL_FEATURES = []\n",
        "LABEL = \"city-mpg\"\n",
        "\n",
        "# Adjust these hyperparameters\n",
        "LEARNING_RATE = 1\n",
        "STEPS = 50\n",
        "\n",
        "linear_regressor = define_linear_regression_model(learning_rate = LEARNING_RATE)\n",
        "linear_regressor = train_model(linear_regressor,\n",
        "                               training_examples[NUMERICAL_FEATURES],\n",
        "                               training_examples[LABEL],\n",
        "                               batch_size=50, steps=STEPS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view_weights_text"
      },
      "source": [
        "Let's look at the model weights learned."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "view_weights_code",
        "colab": {}
      },
      "source": [
        "print(\"weight for price:\", linear_regressor.get_variable_value(\n",
        "    \"linear/linear_model/price/weights\")[0])\n",
        "print(\"weight for highway-mpg:\", linear_regressor.get_variable_value(\n",
        "    \"linear/linear_model/highway-mpg/weights\")[0])\n",
        "print(\"bias:\",  linear_regressor.get_variable_value(\"linear/linear_model/bias_weights\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "think_about_weights_exercise"
      },
      "source": [
        "### Think about these questions \n",
        "\n",
        "* Look at the weight for the two variables.  Do they match what you'd expect to see?\n",
        "* Given that `highway-mpg` is well correlated with `city-mpg`, what is it you see in the histograms that might explain why it was hard to train the model?\n",
        "* For linear regression it is important that all of the features are roughly in the same range so that a priori they are treated as equally important.  How does the range of the price compare to the highway mpg, and what effect might this have when training the model?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "think_about_weights_code_box",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "TYPE YOUR ANSWERS TO THE QUESTIONS HERE\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "exercise_linear_scaling_text"
      },
      "source": [
        "## Exercise: Write a Linear Scaling Function (1 point)\n",
        "\n",
        "There are two characteristics we'd like of numerical features when used together to train a linear model\n",
        "* The range of the features is roughly the same\n",
        "* To the extent possible the histogram of the features kind of resembles a bell curve.  Sometimes the data will fit this very well and other times it won't.\n",
        "\n",
        "As you've already seen in the code, you can take a Pandas column (e.g. `car_data['price']`) and find the min value with `car_data['price'].min()` and likewise find the max with `car_data['price'].max()`. Note that you can use a lambda function to apply `f(x)` to all entries `x` in a Pandas column `feature` using.\n",
        "```\n",
        "   feature.apply(lambda x: f(x))\n",
        "```\n",
        "\n",
        "To provide an example of feature transformation, we have provided an implementation for log scaling.  Note that we take the log of x+1 for column value of x so that we are always taking the log of a number greater than 0 since log 0 is not defined. In this data all values are at least 0, so log(x+1) is well defined.\n",
        "\n",
        "You are to complete the implementation of `linear_scale`, in which you simply stretch/compress and shift the features linearly to fall into the interval [0,1]. The minimum value that occurs will map to 0, the maximum value that occurs will map to 1, (min + max)/2 will map to 0.5, and so on.  You will need to make sure that your output from `linear_scale` is a real number (versus an integer). Be sure to test your function on some examples.  For example if the input series originally had values going from 10 to 20, then after applying linear scale 10 should map to 0.0, 11 should map to 0.1, 12 should map to 0.2, ... and so on with 20 mapping to 1.0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "exercise_linear_scaling_code_box",
        "colab": {}
      },
      "source": [
        "# Perform log scaling\n",
        "def log_scale(series):\n",
        "  return series.apply(lambda x:math.log(x+1.0))\n",
        "\n",
        "# Linearly rescales to the range [0, 1]\n",
        "# You need to write this function.  Right now it just returns the same series.\n",
        "def linear_scale(series):\n",
        "  # add any additional lines of code needed\n",
        "  return series.apply(lambda x: x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "test_scaling_text"
      },
      "source": [
        "**Test your scaling procedure** with the following code block that applies these two scaling methods to `price` and `highway-mpg` and then draws a histogram for each."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "test_scaling_code",
        "colab": {}
      },
      "source": [
        "def draw_histograms(feature_name):\n",
        "  plt.figure(figsize=(20, 4))\n",
        "  plt.subplot(1, 3, 1)\n",
        "  plt.title(feature_name)\n",
        "  histogram = car_data[feature_name].hist(bins=50)\n",
        "\n",
        "  plt.subplot(1, 3, 2)\n",
        "  plt.title(\"linear_scaling\")\n",
        "  scaled_features = pd.DataFrame()\n",
        "  scaled_features[feature_name] = linear_scale(car_data[feature_name])\n",
        "  histogram = scaled_features[feature_name].hist(bins=50)\n",
        "  \n",
        "  plt.subplot(1, 3, 3)\n",
        "  plt.title(\"log scaling\")\n",
        "  log_normalized_features = pd.DataFrame()\n",
        "  log_normalized_features[feature_name] = log_scale(car_data[feature_name])\n",
        "  histogram = log_normalized_features[feature_name].hist(bins=50)\n",
        "  \n",
        "draw_histograms('price')\n",
        "draw_histograms(\"highway-mpg\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "train_with_transformed_features_text"
      },
      "source": [
        "##Exercise: Training the Model Using the Transformed Features (2 points)\n",
        "\n",
        "Modify `prepare_features` to apply linear scaling to `price` and `highway-mpg` and then train the best model you can. **Do not modify the target feature so that the RMSE can be compared to the model you trained in Task 2 and also you want your predictions to be in the correct range**.\n",
        "\n",
        "NOTE: It is possible that if your learning rate is too high you will converge to a solution that is not optimal since you are overshotting and then undershooting the best feature weights as you get close to the optimal solution.  So when looking at the scatter plot, if you converge to a model that is not good, try a slightly smaller learning rate.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "train_with_transformed_features_code",
        "colab": {}
      },
      "source": [
        "def prepare_features(dataframe):\n",
        "  \"\"\"Prepares the features for provided dataset.\n",
        "\n",
        "  Args:\n",
        "    dataframe: A Pandas DataFrame expected to contain data from the\n",
        "      desired data set.\n",
        "  Returns:\n",
        "    A new dataFrame that contains the features to be used for the model.\n",
        "  \"\"\"\n",
        "  processed_features = dataframe.copy()\n",
        "\n",
        "  # Fill in what you want to do here.\n",
        "  \n",
        "  return processed_features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view_transformed_histograms_text"
      },
      "source": [
        "Let's look at the histograms after the feature transformations to ensure it looks as expected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "view_transformed_histograms",
        "colab": {}
      },
      "source": [
        "training_examples = prepare_features(car_data)\n",
        "mpg = training_examples['highway-mpg']\n",
        "price = training_examples['price']\n",
        "plt.hist([mpg, price], bins=50, label=[\"mpg\",\"price\"])\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "train_two_variable_model_text"
      },
      "source": [
        "Now let's train a model.  You should find that you can get a lot better model after the feature transformations have been applied."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "train_two_variable_model_code",
        "colab": {}
      },
      "source": [
        "NUMERICAL_FEATURES = [\"price\", \"highway-mpg\"]\n",
        "CATEGORICAL_FEATURES = []\n",
        "LABEL = \"city-mpg\"\n",
        "\n",
        "# Fill in the rest of the code block to train a model and view the model\n",
        "# weights."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "other_transformations_text"
      },
      "source": [
        "You are welcome to also explore using other feature transformations such as log scaling or linear scaling applied after log scaling.  However, for this simple problem, linear scaling is sufficient."
      ]
    }
  ]
}